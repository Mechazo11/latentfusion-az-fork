{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Estimation\n",
    "\n",
    "This notebook shows the full pipeline for estimating 6D pose of an object given \n",
    "RGB image $\\mathcal{I}$ and matched depth map $\\mathcal{D}$ and Object segmentation mask $\\mathcal{M}$.\n",
    "\n",
    "Major TODOS are as follows\n",
    "* [ ] How we can pass a single reference image with its depth and segmentation mask and recover pose?\n",
    "* [ ] Why .obj file was required and can we recover the same from a .ply point cloud?\n",
    "* [ ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.V.I notes\n",
    "* latenfusion.recon.models.Photographer: Takes in 3D latent space representation of an object and generates an image of that object\n",
    "* latentfusion.recon.models.Sculptor: Takes 2D input data (RGB image, depth and segmentation masks) and\n",
    "forms the volex-based 3D latent representation as described in Section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how this pointcloud looks like, use Cloud compare\n",
    "```/usr/bin/CloudCompare```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import numpy as np\n",
    "import torch\n",
    "# import torch.utils.model_zoo\n",
    "import math\n",
    "\n",
    "# LatentFusion\n",
    "from latentfusion.recon.inference import Observation\n",
    "from latentfusion.datasets.realsense import RealsenseDataset\n",
    "import latentfusion.visualization as viz\n",
    "from latentfusion.augment import gan_denormalize\n",
    "from latentfusion import meshutils\n",
    "from latentfusion import augment\n",
    "from latentfusion.recon.inference import LatentFusionModel\n",
    "from latentfusion.three.orientation import evenly_distributed_quats\n",
    "import latentfusion.pose.estimation as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup global environment\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "MOPED_PATH = Path('datasets/moped')\n",
    "num_ref_views = 8 # How many reference images to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "checkpoint = torch.load('weights/latentfusion-release.pth', weights_only=False)\n",
    "model = LatentFusionModel.from_checkpoint(checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = 'toy_plane' # Name of the object\n",
    "frame_idx = 20\n",
    "\n",
    "# Define path variables\n",
    "# TODO check if we can get away without using the ground truth\n",
    "input_scene_dir = MOPED_PATH / object_id / 'reference' # Ground-truth\n",
    "target_scene_dir = MOPED_PATH / object_id / 'evaluation' # Input\n",
    "\n",
    "# Why are we using object's point cloud?\n",
    "pointcloud_path = input_scene_dir / 'integrated_registered_processed.obj'\n",
    "obj = meshutils.Object3D(pointcloud_path)\n",
    "pointcloud = torch.tensor(obj.vertices, dtype=torch.float32) # May not be used anywhere?\n",
    "diameter = obj.bounding_diameter\n",
    "object_scale = 1.0 / diameter\n",
    "object_scale_to_meters = 1.0 / object_scale\n",
    "\n",
    "print(f\"diameter: {diameter}\")\n",
    "print(f\"object_scale: {object_scale}\")\n",
    "print(f\"object_scale_to_meters: {object_scale_to_meters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find out why we need both .obj file and how it was made?\n",
    "# TODO the function of RealsenseDataset class??\n",
    "# Make listof paths\n",
    "input_paths = [x for x in input_scene_dir.iterdir() if x.is_dir()]\n",
    "#print(input_paths[:2]) # [PosixPath('datasets/moped/toy_plane/reference/06')]\n",
    "input_dataset = RealsenseDataset(input_paths,\n",
    "                                 image_scale=1.0,\n",
    "                                 object_scale=object_scale,\n",
    "                                 odometry_type='open3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_paths = sorted([x for x in target_scene_dir.iterdir() if x.is_dir()])\n",
    "target_dataset = RealsenseDataset(target_paths,\n",
    "                                  image_scale=1.0,\n",
    "                                  object_scale=object_scale, # Can we just use 1.0 here??\n",
    "                                  odometry_type='open3d',\n",
    "                                  use_registration=True)\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch input and target observations\n",
    "# TODO what is Observation.from_dataset doing?\n",
    "input_obs = Observation.from_dataset(input_dataset, inds=input_dataset.sample_evenly(num_ref_views))\n",
    "target_obs = Observation.from_dataset(target_dataset, inds=list(range(len(target_dataset)))[frame_idx:frame_idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess observations\n",
    "# TODO what does model.preprocess_observation do?\n",
    "input_obs_pp = model.preprocess_observation(input_obs)\n",
    "input_obs_pp_gt = model.preprocess_observation(input_obs)\n",
    "target_obs_pp = model.preprocess_observation(target_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create latent representation of the new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_obj = model.build_latent_object(input_obs_pp)\n",
    "\n",
    "    # Visualize prediction.\n",
    "    camera = input_obs_pp.camera.clone()\n",
    "    y, z = model.render_latent_object(z_obj, camera.to(device))\n",
    "\n",
    "# This is the reconstruction error. But for completely unseen objects, we will not be able to do this.\n",
    "recon_error = (y['depth'].detach().cpu() - input_obs_pp_gt.depth).abs()\n",
    "print('recon_error', recon_error.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: For given reference, find coarse estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a latentfusion.pose.estimation.CrossEntropyPoseEstimator model \n",
    "coarse_estimator = pe.load_from_config('configs/cross_entropy_latent.toml', model, return_camera_history=False, verbose=False)\n",
    "\n",
    "# Given an object's 3D latent representation and input vector x = {I,D,M}, find camera poses that best describes the object?\n",
    "# (Camera): One or multiple cameras representing the estimated pose.\n",
    "coarse_camera = coarse_estimator.estimate(z_obj, target_obs[0]) \n",
    "camera_zoom = coarse_camera.zoom(None, model.camera_dist, model.input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: For the coarse estimates, obtain fine estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
