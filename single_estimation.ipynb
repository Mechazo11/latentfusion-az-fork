{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Estimation\n",
    "\n",
    "This notebook shows the full pipeline for estimating 6D pose of an object given \n",
    "RGB image $\\mathcal{I}$ and matched depth map $\\mathcal{D}$ and Object segmentation mask $\\mathcal{M}$.\n",
    "\n",
    "Major TODOS are as follows\n",
    "* [ ] How we can pass a single reference image with its depth and segmentation mask and recover pose?\n",
    "* [ ] Why .obj file was required and can we recover the same from a .ply point cloud?\n",
    "* [ ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how this pointcloud looks like, use Cloud compare\n",
    "```/usr/bin/CloudCompare```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-03 22:59.35\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mcould not import PCL          \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.pointcloud\u001b[0m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import numpy as np\n",
    "import torch\n",
    "# import torch.utils.model_zoo\n",
    "import math\n",
    "\n",
    "# LatentFusion\n",
    "from latentfusion.recon.inference import Observation\n",
    "from latentfusion.datasets.realsense import RealsenseDataset\n",
    "import latentfusion.visualization as viz\n",
    "from latentfusion.augment import gan_denormalize\n",
    "from latentfusion import meshutils\n",
    "from latentfusion import augment\n",
    "from latentfusion.recon.inference import LatentFusionModel\n",
    "from latentfusion.three.orientation import evenly_distributed_quats\n",
    "import latentfusion.pose.estimation as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup global environment\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "MOPED_PATH = Path('datasets/moped')\n",
    "num_ref_views = 8 # How many reference images to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-03 22:59.39\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded model                  \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.recon.inference\u001b[0m]\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m200\u001b[0m \u001b[36mname\u001b[0m=\u001b[35mshapenet,no_mask_morph,fixed_eqlr,256,mask,depth,in_mask,mask_noise_p=0.25,sm=nearest,fuser=gru-branched_20200509_10h19m10s-branched_20200509_10h42m53s-branched_20200509_10h46m53s-branched_20200509_10h48m49s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "checkpoint = torch.load('weights/latentfusion-release.pth', weights_only=False)\n",
    "model = LatentFusionModel.from_checkpoint(checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter: 0.3425245885532891\n",
      "object_scale: 2.919498434327504\n",
      "object_scale_to_meters: 0.3425245885532891\n"
     ]
    }
   ],
   "source": [
    "object_id = 'toy_plane' # Name of the object\n",
    "frame_idx = 20\n",
    "\n",
    "# Define path variables\n",
    "# TODO check if we can get away without using the ground truth\n",
    "input_scene_dir = MOPED_PATH / object_id / 'reference' # Ground-truth\n",
    "target_scene_dir = MOPED_PATH / object_id / 'evaluation' # Input\n",
    "\n",
    "# Why are we using object's point cloud?\n",
    "pointcloud_path = input_scene_dir / 'integrated_registered_processed.obj'\n",
    "obj = meshutils.Object3D(pointcloud_path)\n",
    "pointcloud = torch.tensor(obj.vertices, dtype=torch.float32) # May not be used anywhere?\n",
    "diameter = obj.bounding_diameter\n",
    "object_scale = 1.0 / diameter\n",
    "object_scale_to_meters = 1.0 / object_scale\n",
    "\n",
    "print(f\"diameter: {diameter}\")\n",
    "print(f\"object_scale: {object_scale}\")\n",
    "print(f\"object_scale_to_meters: {object_scale_to_meters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/02/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/02/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/06/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/06/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/01/registration/manual.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/01/registration/manual.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/03/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/03/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/05/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/reference/05/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.53\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mfiltered points               \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mnum_filtered\u001b[0m=\u001b[35m13553\u001b[0m \u001b[36mnum_valid\u001b[0m=\u001b[35m121985\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO find out why we need both .obj file and how it was made?\n",
    "# TODO the function of RealsenseDataset class??\n",
    "# Make listof paths\n",
    "input_paths = [x for x in input_scene_dir.iterdir() if x.is_dir()]\n",
    "#print(input_paths[:2]) # [PosixPath('datasets/moped/toy_plane/reference/06')]\n",
    "input_dataset = RealsenseDataset(input_paths,\n",
    "                                 image_scale=1.0,\n",
    "                                 object_scale=object_scale,\n",
    "                                 odometry_type='open3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/00/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/00/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/01/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/01/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/02/registration/manual.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/02/registration/manual.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/03/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/03/registration/registration.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/04/registration/manual.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1musing registration            \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35mdatasets/moped/toy_plane/evaluation/04/registration/manual.json\u001b[0m\n",
      "\u001b[2m2024-10-03 23:01.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mfiltered points               \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.datasets.realsense\u001b[0m]\u001b[0m \u001b[36mnum_filtered\u001b[0m=\u001b[35m7667\u001b[0m \u001b[36mnum_valid\u001b[0m=\u001b[35m69001\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "target_paths = sorted([x for x in target_scene_dir.iterdir() if x.is_dir()])\n",
    "target_dataset = RealsenseDataset(target_paths,\n",
    "                                  image_scale=1.0,\n",
    "                                  object_scale=object_scale, # Can we just use 1.0 here??\n",
    "                                  odometry_type='open3d',\n",
    "                                  use_registration=True)\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigerwife/miniforge3/envs/latentfusion/lib/python3.10/site-packages/torch/utils/data/sampler.py:65: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    }
   ],
   "source": [
    "# Batch input and target observations\n",
    "# TODO what is Observation.from_dataset doing?\n",
    "input_obs = Observation.from_dataset(input_dataset, inds=input_dataset.sample_evenly(num_ref_views))\n",
    "target_obs = Observation.from_dataset(target_dataset, inds=list(range(len(target_dataset)))[frame_idx:frame_idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigerwife/latentfusion-az-fork/latentfusion/modules/geometry.py:351: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  grids = bboxes_to_grid(boxes, in_size, out_size)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess observations\n",
    "# TODO what does model.preprocess_observation do?\n",
    "input_obs_pp = model.preprocess_observation(input_obs)\n",
    "input_obs_pp_gt = model.preprocess_observation(input_obs)\n",
    "target_obs_pp = model.preprocess_observation(target_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create latent representation of the new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_error 0.043014854192733765\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z_obj = model.build_latent_object(input_obs_pp)\n",
    "\n",
    "    # Visualize prediction.\n",
    "    camera = input_obs_pp.camera.clone()\n",
    "    y, z = model.render_latent_object(z_obj, camera.to(device))\n",
    "\n",
    "# This is the reconstruction error. But for completely unseen objects, we will not be able to do this.\n",
    "recon_error = (y['depth'].detach().cpu() - input_obs_pp_gt.depth).abs()\n",
    "print('recon_error', recon_error.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: For given reference, find coarse estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: For the coarse estimates, obtain fine estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
