{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Estimation\n",
    "\n",
    "This notebook shows the full pipeline for estimating 6D pose of an object given \n",
    "RGB image $\\mathcal{I}$ and matched depth map $\\mathcal{D}$ and Object segmentation mask $\\mathcal{M}$.\n",
    "\n",
    "This is shown the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-03 18:50.02\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mcould not import PCL          \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.pointcloud\u001b[0m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import numpy as np\n",
    "import torch\n",
    "# import torch.utils.model_zoo\n",
    "import math\n",
    "\n",
    "# LatentFusion\n",
    "from latentfusion.recon.inference import Observation\n",
    "from latentfusion.datasets.realsense import RealsenseDataset\n",
    "import latentfusion.visualization as viz\n",
    "from latentfusion.augment import gan_denormalize\n",
    "from latentfusion import meshutils\n",
    "from latentfusion import augment\n",
    "from latentfusion.recon.inference import LatentFusionModel\n",
    "from latentfusion.three.orientation import evenly_distributed_quats\n",
    "import latentfusion.pose.estimation as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup global environment\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "MOPED_PATH = Path('datasets/moped')\n",
    "num_ref_views = 8 # How many reference images to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62246/3030555503.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('weights/latentfusion-release.pth')\n",
      "\u001b[2m2024-10-03 18:50.10\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded model                  \u001b[0m [\u001b[0m\u001b[1m\u001b[34mlatentfusion.recon.inference\u001b[0m]\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m200\u001b[0m \u001b[36mname\u001b[0m=\u001b[35mshapenet,no_mask_morph,fixed_eqlr,256,mask,depth,in_mask,mask_noise_p=0.25,sm=nearest,fuser=gru-branched_20200509_10h19m10s-branched_20200509_10h42m53s-branched_20200509_10h46m53s-branched_20200509_10h48m49s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "checkpoint = torch.load('weights/latentfusion-release.pth')\n",
    "model = LatentFusionModel.from_checkpoint(checkpoint, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create latent representation of the new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter: 0.3425245885532891\n",
      "object_scale: 2.919498434327504\n",
      "object_scale_to_meters: 0.3425245885532891\n"
     ]
    }
   ],
   "source": [
    "object_id = 'toy_plane' # \n",
    "test_frame = 20\n",
    "\n",
    "# Define path variables\n",
    "# TODO check if we can get away without using the ground truth\n",
    "input_scene_dir = MOPED_PATH / object_id / 'reference' # Ground-truth\n",
    "target_scene_dir = MOPED_PATH / object_id / 'evaluation' # Input\n",
    "\n",
    "# Why are we using object's point cloud?\n",
    "pointcloud_path = input_scene_dir / 'integrated_registered_processed.obj'\n",
    "obj = meshutils.Object3D(pointcloud_path)\n",
    "pointcloud = torch.tensor(obj.vertices, dtype=torch.float32) # May not be used anywhere?\n",
    "diameter = obj.bounding_diameter\n",
    "object_scale = 1.0 / diameter\n",
    "object_scale_to_meters = 1.0 / object_scale\n",
    "\n",
    "print(f\"diameter: {diameter}\")\n",
    "print(f\"object_scale: {object_scale}\")\n",
    "print(f\"object_scale_to_meters: {object_scale_to_meters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how this pointcloud looks like, use Cloud compare\n",
    "```/usr/bin/CloudCompare```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find out why we need both .obj file and how it was made?\n",
    "# TODO the function of RealsenseDataset class??\n",
    "# Make listof paths\n",
    "input_paths = [x for x in input_scene_dir.iterdir() if x.is_dir()]\n",
    "#print(input_paths[:2]) # [PosixPath('datasets/moped/toy_plane/reference/06')]\n",
    "input_dataset = RealsenseDataset(input_paths,\n",
    "                                 image_scale=1.0,\n",
    "                                 object_scale=object_scale,\n",
    "                                 odometry_type='open3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_paths = sorted([x for x in target_scene_dir.iterdir() if x.is_dir()])\n",
    "target_dataset = RealsenseDataset(target_paths,\n",
    "                                  image_scale=1.0,\n",
    "                                  object_scale=object_scale, # Can we just use 1.0 here??\n",
    "                                  odometry_type='open3d',\n",
    "                                  use_registration=True)\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create latent representation of the new object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: For given reference, find coarse estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: For the coarse estimates, obtain fine estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
